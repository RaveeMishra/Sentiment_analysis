{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile requirements.txt\n",
        "\n",
        "numpy==1.26.4\n",
        "scikit-learn==1.4.2\n",
        "torch==2.2.2\n",
        "transformers==4.41.2\n",
        "datasets==2.18.0\n",
        "tqdm\n",
        "pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRvsbL7gnIgB",
        "outputId": "d8e6eb69-b05a-4334-8868-29f20ff6cc48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li8svxN7nPtS",
        "outputId": "92591457-b093-42f1-d57a-d5c3306c54df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: transformers==4.41.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.41.2)\n",
            "Requirement already satisfied: datasets==2.18.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.67.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 3)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2->-r requirements.txt (line 4)) (12.1.105)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (0.36.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 6)) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 6)) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 6)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 6)) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.18.0->-r requirements.txt (line 6)) (3.13.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 4)) (12.8.93)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 8)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 8)) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 6)) (1.22.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2->-r requirements.txt (line 5)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2->-r requirements.txt (line 5)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2->-r requirements.txt (line 5)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2->-r requirements.txt (line 5)) (2026.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2->-r requirements.txt (line 4)) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score , accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "############################################\n",
        "# CONFIG\n",
        "############################################\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LR = 2e-4\n",
        "MAX_LEN = 128\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "############################################\n",
        "# DATASET\n",
        "############################################\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split=\"train\"):\n",
        "        dataset = load_dataset(\"imdb\")[split]\n",
        "\n",
        "        self.texts = dataset[\"text\"]\n",
        "        self.labels = dataset[\"label\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=MAX_LEN,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def get_loader(split):\n",
        "    return DataLoader(\n",
        "        SentimentDataset(split),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=(split == \"train\"),\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "############################################\n",
        "# MODELS\n",
        "############################################\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab=30522, embed=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab, embed)\n",
        "\n",
        "        self.conv = nn.Conv1d(embed, 256, kernel_size=5)\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, ids):\n",
        "\n",
        "        x = self.embedding(ids)      # (B,L,E)\n",
        "        x = x.permute(0, 2, 1)       # (B,E,L)\n",
        "\n",
        "        x = torch.relu(self.conv(x))\n",
        "        x = self.pool(x).squeeze(-1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab=30522, embed=128, hidden=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab, embed)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed,\n",
        "            hidden,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(hidden * 2, 2)\n",
        "\n",
        "    def forward(self, ids):\n",
        "\n",
        "        x = self.embedding(ids)\n",
        "\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "\n",
        "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        return self.fc(hidden)\n",
        "\n",
        "\n",
        "############################################\n",
        "# METRICS\n",
        "############################################\n",
        "\n",
        "def compute_metrics(logits, labels):\n",
        "    probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision_score(labels, preds),\n",
        "        \"Recall\": recall_score(labels, preds),\n",
        "        \"F1\": f1_score(labels, preds),\n",
        "        \"ROC-AUC\": roc_auc_score(labels, probs),\n",
        "        \"Accuracy\": accuracy_score(labels, preds)\n",
        "    }\n",
        "\n",
        "\n",
        "############################################\n",
        "# TRAINING\n",
        "############################################\n",
        "\n",
        "def evaluate(model, loader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in loader:\n",
        "\n",
        "            ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            labels = batch[\"label\"].to(DEVICE)\n",
        "\n",
        "            logits = model(ids)\n",
        "\n",
        "            all_logits.append(logits)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    metrics = compute_metrics(\n",
        "        torch.cat(all_logits),\n",
        "        torch.cat(all_labels)\n",
        "    )\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "\n",
        "def train(model_type=\"cnn\"):\n",
        "\n",
        "    train_loader = get_loader(\"train\")\n",
        "    test_loader = get_loader(\"test\")\n",
        "\n",
        "    if model_type == \"cnn\":\n",
        "        model = CNNClassifier().to(DEVICE)\n",
        "    else:\n",
        "        model = RNNClassifier().to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        model.train()\n",
        "        loop = tqdm(train_loader)\n",
        "\n",
        "        for batch in loop:\n",
        "\n",
        "            ids = batch[\"input_ids\"].to(DEVICE)\n",
        "            labels = batch[\"label\"].to(DEVICE)\n",
        "\n",
        "            logits = model(ids)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loop.set_description(f\"Epoch {epoch+1}\")\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        evaluate(model, test_loader)\n",
        "\n",
        "    torch.save(model.state_dict(), f\"{model_type}_sentiment.pt\")\n",
        "\n",
        "    print(\"\\nModel saved ✔\")\n",
        "\n",
        "\n",
        "############################################\n",
        "# KMEANS (UNSUPERVISED SENTIMENT)\n",
        "############################################\n",
        "\n",
        "def run_kmeans():\n",
        "\n",
        "    print(\"\\nRunning KMeans clustering...\")\n",
        "\n",
        "    dataset = load_dataset(\"imdb\")[\"train\"]\n",
        "\n",
        "    texts = dataset[\"text\"][:5000]\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        stop_words=\"english\"\n",
        "    )\n",
        "\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "\n",
        "    unique, counts = np.unique(kmeans.labels_, return_counts=True)\n",
        "\n",
        "    print(\"Cluster Distribution:\")\n",
        "    print(dict(zip(unique, counts)))\n",
        "\n",
        "\n",
        "############################################\n",
        "# MAIN\n",
        "############################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"Device:\", DEVICE)\n",
        "\n",
        "    # Choose model: \"cnn\" or \"rnn\"\n",
        "    train(model_type=\"cnn\")\n",
        "\n",
        "    print('*' * 100)\n",
        "\n",
        "    train(model_type=\"rnn\")\n",
        "\n",
        "    print('*' * 100)\n",
        "\n",
        "    # Run unsupervised clustering\n",
        "    run_kmeans()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqEdrJjWmsAE",
        "outputId": "a8f29bcb-b5e8-4396-94dd-ee566d88584e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [00:35<00:00, 22.10it/s, loss=0.473]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Precision: 0.6950\n",
            "Recall: 0.7518\n",
            "F1: 0.7223\n",
            "ROC-AUC: 0.7904\n",
            "Accuracy: 0.7109\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [00:36<00:00, 21.57it/s, loss=0.384]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Precision: 0.7117\n",
            "Recall: 0.8232\n",
            "F1: 0.7634\n",
            "ROC-AUC: 0.8334\n",
            "Accuracy: 0.7448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [00:35<00:00, 21.80it/s, loss=0.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Precision: 0.7409\n",
            "Recall: 0.8182\n",
            "F1: 0.7776\n",
            "ROC-AUC: 0.8541\n",
            "Accuracy: 0.7660\n",
            "\n",
            "Model saved ✔\n",
            "****************************************************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 782/782 [00:40<00:00, 19.29it/s, loss=0.683]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Precision: 0.6581\n",
            "Recall: 0.7370\n",
            "F1: 0.6953\n",
            "ROC-AUC: 0.7405\n",
            "Accuracy: 0.6771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 782/782 [00:39<00:00, 19.65it/s, loss=0.335]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Precision: 0.7546\n",
            "Recall: 0.6897\n",
            "F1: 0.7207\n",
            "ROC-AUC: 0.8157\n",
            "Accuracy: 0.7327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 782/782 [00:41<00:00, 18.81it/s, loss=0.254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Precision: 0.7368\n",
            "Recall: 0.8129\n",
            "F1: 0.7730\n",
            "ROC-AUC: 0.8439\n",
            "Accuracy: 0.7612\n",
            "\n",
            "Model saved ✔\n",
            "****************************************************************************************************\n",
            "\n",
            "Running KMeans clustering...\n",
            "Cluster Distribution:\n",
            "{0: 3044, 1: 1956}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTJN7s7xmr8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Txv9DxP-mr6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKaLgGmZmr4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJKjsIufmryn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}